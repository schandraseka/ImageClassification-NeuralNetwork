import autograd.numpy as np
import autograd
from autograd.util import flatten
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import kaggle
import time
import pandas as pd

# Function to compute classification accuracy
def mean_zero_one_loss(weights, x, y_integers, unflatten):
	(W, b, V, c) = unflatten(weights)
	out = feedForward(W, b, V, c, x)
	pred = np.argmax(out, axis=1)
	return(np.mean(pred != y_integers))

# Feed forward output i.e. L = -O[y] + log(sum(exp(O[j])))
def feedForward(W, b, V, c, train_x):
        hid = np.tanh(np.dot(train_x, W) + b)
        out = np.dot(hid, V) + c
        return out

# Logistic Loss function
def logistic_loss_batch(weights, x, y, unflatten):
	# regularization penalty
        lambda_pen = 10
        # unflatten weights into W, b, V and c respectively 
        (W, b, V, c) = unflatten(weights)
        # Predict output for the entire train data
        out  = feedForward(W, b, V, c, x)
        pred = np.argmax(out, axis=1)
	# True labels
        true = np.argmax(y, axis=1)
        # Mean accuracy
        class_err = np.mean(pred != true)
        # Computing logistic loss with l2 penalization
        logistic_loss = np.sum(-np.sum(out * y, axis=1) + np.log(np.sum(np.exp(out),axis=1))) + lambda_pen * np.sum(weights**2)
        # returning loss. Note that outputs can only be returned in the below format
        return (logistic_loss, [autograd.util.getval(logistic_loss), autograd.util.getval(class_err)])

# Loading the dataset
print('Reading image data ...')
temp = np.load('../../Data/data_train.npz')
train_x = temp['data_train']
temp = np.load('../../Data/labels_train.npz')
train_y_integers = temp['labels_train']
temp = np.load('../../Data/data_test.npz')
test_x = temp['data_test']

train_x -= .5
test_x  -= .5
dims_out = 4
dims_hid = 70
dims_in = train_x.shape[1]
W = np.random.randn(dims_in, dims_hid)
b = np.random.randn(dims_hid)
V = np.random.randn(dims_hid, dims_out)
c = np.random.randn(dims_out)
nTrainSamples = train_x.shape[0]
y = np.zeros((nTrainSamples, dims_out))
y[np.arange(nTrainSamples), train_y_integers] = 1
epsilon = 0.0001
momentum = 0.1
grad_fun = autograd.grad_and_aux(logistic_loss_batch) 
all_weights = (W, b, V, c)
weights, unflatten = flatten(all_weights)
smooth_grad = 0
nEpochs = 1000
for epoch in range(nEpochs):
    weight_gradients, returned_values = grad_fun(weights, train_x, y, unflatten)
    smooth_grad = (1 - momentum) * smooth_grad + momentum * weight_gradients
    weights = weights - epsilon * smooth_grad
W,b,V,c = unflatten(weights)
ycap = feedForward(W,b,V,c, test_x)
predicted_ys = [] 
for i in range(0, ycap.shape[0]): 
    predicted_ys.append(np.argmax(ycap[i]))


# Output file location
file_name = r'../Predictions/NeuralNetwork.csv'
# Writing output in Kaggle format
print('Predictions exported to ', file_name)
kaggle.kaggleize(np.array(predicted_ys), file_name)
